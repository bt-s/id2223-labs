{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning With Spark ML\n",
    "In this lab assignment, you will complete a project by going through the following steps:\n",
    "1. Get the data.\n",
    "2. Discover the data to gain insights.\n",
    "3. Prepare the data for Machine Learning algorithms.\n",
    "4. Select a model and train it.\n",
    "5. Fine-tune your model.\n",
    "6. Present your solution.\n",
    "\n",
    "As a dataset, we use the California Housing Prices dataset from the StatLib repository. This dataset was based on data from the 1990 California census. The dataset has the following columns\n",
    "1. `longitude`: a measure of how far west a house is (a higher value is farther west)\n",
    "2. `latitude`: a measure of how far north a house is (a higher value is farther north)\n",
    "3. `housing_,median_age`: median age of a house within a block (a lower number is a newer building)\n",
    "4. `total_rooms`: total number of rooms within a block\n",
    "5. `total_bedrooms`: total number of bedrooms within a block\n",
    "6. `population`: total number of people residing within a block\n",
    "7. `households`: total number of households, a group of people residing within a home unit, for a block\n",
    "8. `median_income`: median income for households within a block of houses\n",
    "9. `median_house_value`: median house value for households within a block\n",
    "10. `ocean_proximity`: location of the house w.r.t ocean/sea\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: String = 2.4.4\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://130.229.191.72:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1574686600672)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.ml.util._\r\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors, Matrix, Matrices}\r\n",
       "import org.apache.spark.ml.stat.Correlation\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\r\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.feature.{Imputer, VectorAssembler, StandardScaler, StringIndexer, OneHotEncoderEstimator}\r\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\r\n",
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, LogisticRegression}\r\n",
       "import org.apache.spark.ml.regression.{LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor}\r\n",
       "import org.apache.spark.ml..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.util._\n",
    "\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors, Matrix, Matrices}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.feature.{Imputer, VectorAssembler, StandardScaler, StringIndexer, OneHotEncoderEstimator}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, LogisticRegression}\n",
    "import org.apache.spark.ml.regression.{LinearRegression, DecisionTreeRegressor, RandomForestRegressor, GBTRegressor}\n",
    "import org.apache.spark.ml.evaluation.{RegressionEvaluator, BinaryClassificationEvaluator}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, Row}\n",
    "import org.apache.spark.ml.util.Identifiable\n",
    "import org.apache.spark.ml.Transformer\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "import org.apache.spark.sql.types.{StructType}\n",
    "import org.apache.spark.sql.functions.{col, udf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get the data\n",
    "Let's start the lab by loading the dataset. The can find the dataset at `data/housing.csv`. To infer column types automatically, when you are reading the file, you need to set `inferSchema` to true. Moreover enable the `header` option to read the columns' name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "housing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housing = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferschema\", \"true\")\n",
    "    .load(\"data/housing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Discover the data to gain insights\n",
    "Now it is time to take a look at the data. In this step we are going to take a look at the data a few different ways:\n",
    "* See the schema and dimension of the dataset\n",
    "* Look at the data itself\n",
    "* Statistical summary of the attributes\n",
    "* Breakdown of the data by the categorical attribute variable\n",
    "* Find the correlation among different attributes\n",
    "* Make new attributes by combining existing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Schema and dimension\n",
    "Print the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 20640\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at the data\n",
    "Print the first five records of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|-122.23  |37.88   |41.0              |880.0      |129.0         |322.0     |126.0     |8.3252       |452600.0          |NEAR BAY       |\n",
      "|-122.22  |37.86   |21.0              |7099.0     |1106.0        |2401.0    |1138.0    |8.3014       |358500.0          |NEAR BAY       |\n",
      "|-122.24  |37.85   |52.0              |1467.0     |190.0         |496.0     |177.0     |7.2574       |352100.0          |NEAR BAY       |\n",
      "|-122.25  |37.85   |52.0              |1274.0     |235.0         |558.0     |219.0     |5.6431       |341300.0          |NEAR BAY       |\n",
      "|-122.25  |37.85   |52.0              |1627.0     |280.0         |565.0     |259.0     |3.8462       |342200.0          |NEAR BAY       |\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records with population more than 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 23\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.filter($\"population\" > 10000).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Statistical summary\n",
    "Print a summary of the table statistics for the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. You can use the `describe` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|housing_median_age|       total_rooms|median_house_value|        population|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|\n",
      "|   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n",
      "| stddev| 12.58555761211163|2181.6152515827944|115395.61587441359|  1132.46212176534|\n",
      "|    min|               1.0|               2.0|           14999.0|               3.0|\n",
      "|    max|              52.0|           39320.0|          500001.0|           35682.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.describe(\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the maximum age (`housing_median_age`), the minimum number of rooms (`total_rooms`), and the average of house values (`median_house_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|max(housing_median_age)|\n",
      "+-----------------------+\n",
      "|                   52.0|\n",
      "+-----------------------+\n",
      "\n",
      "+----------------+\n",
      "|min(total_rooms)|\n",
      "+----------------+\n",
      "|             2.0|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------+\n",
      "|avg(median_house_value)|\n",
      "+-----------------------+\n",
      "|     206855.81690891474|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.agg(max(\"housing_median_age\")).show()\n",
    "housing.agg(min(\"total_rooms\")).show()\n",
    "housing.agg(avg(\"median_house_value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Breakdown the data by categorical data\n",
    "Print the number of houses in different areas (`ocean_proximity`), and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|         ISLAND|    5|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.groupBy(\"ocean_proximity\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average value of the houses (`median_house_value`) in different areas (`ocean_proximity`), and call the new column `avg_value` when print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.groupBy(\"ocean_proximity\")\n",
    "    .agg(avg(\"median_house_value\"))\n",
    "    .withColumnRenamed(\"avg(median_house_value)\",  \"avg_value\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above question in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"SELECT ocean_proximity, avg(median_house_value) AS avg_value FROM df GROUP BY ocean_proximity\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Correlation among attributes\n",
    "Print the correlation among the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. To do so, first you need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, you can use the `VectorAssembler` Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|            features|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|[41.0,880.0,45260...|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|[21.0,7099.0,3585...|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|[52.0,1467.0,3521...|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|[52.0,1274.0,3413...|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|[52.0,1627.0,3422...|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_a48a38ef29c6\n",
       "housingAttrs: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va = new VectorAssembler()\n",
    "    .setInputCols(Array(\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val housingAttrs = va.transform(housing)\n",
    "\n",
    "housingAttrs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard correlation coefficient:\n",
      " 1.0                   -0.36126220122231784  0.10562341249318154   -0.2962442397735293   \n",
      "-0.36126220122231784  1.0                   0.13415311380654338   0.8571259728659772    \n",
      "0.10562341249318154   0.13415311380654338   1.0                   -0.02464967888891235  \n",
      "-0.2962442397735293   0.8571259728659772    -0.02464967888891235  1.0                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "coeff: org.apache.spark.ml.linalg.Matrix =\n",
       "1.0                   -0.36126220122231784  0.10562341249318154   -0.2962442397735293\n",
       "-0.36126220122231784  1.0                   0.13415311380654338   0.8571259728659772\n",
       "0.10562341249318154   0.13415311380654338   1.0                   -0.02464967888891235\n",
       "-0.2962442397735293   0.8571259728659772    -0.02464967888891235  1.0\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Row(coeff: Matrix) = Correlation.corr(housingAttrs, \"features\").head\n",
    "\n",
    "println(s\"The standard correlation coefficient:\\n ${coeff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Combine and make new attributes\n",
    "Now, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful, if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, add the three new columns to the dataset as below. We will call the new dataset the `housingExtra`.\n",
    "```\n",
    "rooms_per_household = total_rooms / households\n",
    "bedrooms_per_room = total_bedrooms / total_rooms\n",
    "population_per_household = population / households\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------+\n",
      "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+-------------------+-------------------+------------------------+\n",
      "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "housingCol1: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n",
       "housingCol2: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\n",
       "housingExtra: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housingCol1 = housing.withColumn(\"rooms_per_household\", expr(\"total_rooms/households\"))\n",
    "val housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\", expr(\"total_bedrooms/total_rooms\"))\n",
    "val housingExtra = housingCol2.withColumn(\"population_per_household\", expr(\"population/households\"))\n",
    "\n",
    "housingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prepare the data for Machine Learning algorithms\n",
    "Before going through the Machine Learning steps, let's first rename the label column from `median_house_value` to `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "renamedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val renamedHousing = housingExtra.withColumnRenamed(\"median_house_value\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to separate the numerical attributes from the categorical attribute (`ocean_proximity`) and keep their column names in two different lists. Moreover, sice we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel: String = label\n",
       "colCat: String = ocean_proximity\n",
       "colNum: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, rooms_per_household, bedrooms_per_room, population_per_household)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Label column\n",
    "val colLabel = \"label\"\n",
    "\n",
    "// Categorical columns\n",
    "val colCat = \"ocean_proximity\"\n",
    "\n",
    "// Numerical columns\n",
    "val colNum = renamedHousing.columns.filter(_ != colLabel).filter(_ != colCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepare continuous attributes\n",
    "### Data cleaning\n",
    "Most Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continues attributes, listed in `colNum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for longitude : 0\n",
      "Missing values for latitude : 0\n",
      "Missing values for housing_median_age : 0\n",
      "Missing values for total_rooms : 0\n",
      "Missing values for total_bedrooms : 207\n",
      "Missing values for population : 0\n",
      "Missing values for households : 0\n",
      "Missing values for median_income : 0\n",
      "Missing values for rooms_per_household : 0\n",
      "Missing values for bedrooms_per_room : 207\n",
      "Missing values for population_per_household : 0\n"
     ]
    }
   ],
   "source": [
    "for (c <- colNum) {\n",
    "    val missingCount = renamedHousing.filter(renamedHousing(c).isNull || \n",
    "            renamedHousing(c) === \"\" || renamedHousing(c).isNaN).count() \n",
    "    printf(\"Missing values for \" + c + \" : \" + missingCount + \"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observerd above, the `total_bedrooms` and `bedrooms_per_room` attributes have some missing values. One way to take care of missing values is to use the `Imputer` Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an `Imputer` instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|total_bedrooms|  bedrooms_per_room|\n",
      "+--------------+-------------------+\n",
      "|         129.0|0.14659090909090908|\n",
      "|        1106.0|0.15579659106916466|\n",
      "|         190.0|0.12951601908657123|\n",
      "|         235.0|0.18445839874411302|\n",
      "|         280.0| 0.1720958819913952|\n",
      "+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "imputer: org.apache.spark.ml.feature.Imputer = imputer_2aa1739408dc\n",
       "imputedHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imputer = new Imputer()\n",
    "    .setInputCols(Array(\"total_bedrooms\", \"bedrooms_per_room\"))\n",
    "    .setOutputCols(Array(\"total_bedrooms\", \"bedrooms_per_room\"))    \n",
    "    .setStrategy(\"median\")\n",
    "\n",
    "val imputedHousing = imputer\n",
    "    .fit(renamedHousing.select(colNum.head, colNum.tail: _*))\n",
    "    .transform(renamedHousing.select(colNum.head, colNum.tail: _*))\n",
    "\n",
    "imputedHousing.select(\"total_bedrooms\", \"bedrooms_per_room\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude Missing values: 0\n",
      "latitude Missing values: 0\n",
      "housing_median_age Missing values: 0\n",
      "total_rooms Missing values: 0\n",
      "total_bedrooms Missing values: 0\n",
      "population Missing values: 0\n",
      "households Missing values: 0\n",
      "median_income Missing values: 0\n",
      "rooms_per_household Missing values: 0\n",
      "bedrooms_per_room Missing values: 0\n",
      "population_per_household Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "// Sanity check\n",
    "for (c <- imputedHousing.columns) {\n",
    "    printf(c + \" Missing values: \")\n",
    "    val missingCount = imputedHousing.filter(imputedHousing(c).isNull || \n",
    "            imputedHousing(c) === \"\" || imputedHousing(c).isNaN).count() \n",
    "    println(missingCount)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attributes is generally not required.\n",
    "\n",
    "One way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. To do this, we can use the `StandardScaler` Estimator. To use `StandardScaler`, again we need to convert all the numerical attributes into a big vector of features using `VectorAssembler`, and then call `StandardScaler` on that vactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|scaled                                                                                                                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-61.00726959606955,17.734477624640412,3.2577023016083064,0.40337085073160667,0.30758821710917267,0.2843362208866199,0.3295584480852433,4.382095394195227,2.8228125480951665,2.5405867237343416,0.24605655309533123]|\n",
      "|[-61.002278409814444,17.725114120086744,1.668579227653035,3.2540109878905406,2.637151690873992,2.1201592122632746,2.9764882057222772,4.369567902917918,2.5213017566153497,2.700131633864973,0.2031418986718504]     |\n",
      "|[-61.012260782324645,17.720432367809913,4.131719992283705,0.6724375432082579,0.45303690892048687,0.4379837439744208,0.4629511532626037,3.820042655291457,3.349860792340824,2.244659512945694,0.2698099860028393]    |\n",
      "|[-61.01725196857974,17.720432367809913,4.131719992283705,0.5839709816273487,0.5603351241911285,0.4927317119712234,0.5728039692910182,2.970331345671345,2.3512306012371758,3.1968732702241724,0.2453238057662802]    |\n",
      "|[-61.01725196857974,17.720432367809913,4.131719992283705,0.7457776978867319,0.6676333394617702,0.4989129341644108,0.6774256988418891,2.024505754234575,2.5389707703782265,2.9826168328456415,0.21003820253311387]   |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_db9e3cdf4f84\n",
       "featuredHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_8baab90f410a\n",
       "scaledHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va = new VectorAssembler()\n",
    "    .setInputCols(colNum)\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val featuredHousing = va.transform(imputedHousing)\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "    .setInputCol(\"features\")\n",
    "    .setOutputCol(\"scaled\")\n",
    "\n",
    "val scaledHousing = scaler\n",
    "    .fit(featuredHousing)\n",
    "    .transform(featuredHousing)\n",
    "\n",
    "scaledHousing.select(\"scaled\").show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Prepare categorical attributes\n",
    "After imputing and scaling the continuous attributes, we should take care of the categorical attributes. Let's first print the number of distict values of the categirical attribute `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|         ISLAND|    5|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renamedHousing.groupBy(\"ocean_proximity\")\n",
    "    .count()\n",
    "    .sort(desc(\"count\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String indexer\n",
    "Most Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute `ocean_proximity` to numbers. To do so, we can use the `StringIndexer` that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|ocean_proximity_ints|\n",
      "+--------------------+\n",
      "|3.0                 |\n",
      "|3.0                 |\n",
      "|3.0                 |\n",
      "|3.0                 |\n",
      "|3.0                 |\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_176b0dc2cc57\n",
       "idxHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"ocean_proximity\")\n",
    "    .setOutputCol(\"ocean_proximity_ints\")\n",
    "\n",
    "val idxHousing = indexer\n",
    "    .fit(renamedHousing)\n",
    "    .transform(renamedHousing)\n",
    "\n",
    "idxHousing.select(\"ocean_proximity_ints\")show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this numerical data in any Machine Learning algorithm. You can look at the mapping that this encoder has learned using the `labels` method: \"<1H OCEAN\" is mapped to 0, \"INLAND\" is mapped to 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[String] = Array(<1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.fit(renamedHousing).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "Now, convert the label indices built in the last step into one-hot vectors. To do this, you can take advantage of the `OneHotEncoderEstimator` Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------------------+\n",
      "|ocean_proximity|ocean_proximity_ints|ocean_proximity_one_hot|\n",
      "+---------------+--------------------+-----------------------+\n",
      "|       NEAR BAY|                 3.0|          (4,[3],[1.0])|\n",
      "|       NEAR BAY|                 3.0|          (4,[3],[1.0])|\n",
      "|       NEAR BAY|                 3.0|          (4,[3],[1.0])|\n",
      "|       NEAR BAY|                 3.0|          (4,[3],[1.0])|\n",
      "|       NEAR BAY|                 3.0|          (4,[3],[1.0])|\n",
      "+---------------+--------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_2a89a544bf5c\n",
       "ohHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 13 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoderEstimator()\n",
    "    .setInputCols(Array(\"ocean_proximity_ints\"))\n",
    "    .setOutputCols(Array(\"ocean_proximity_one_hot\"))\n",
    "\n",
    "val ohHousing = encoder.\n",
    "    fit(idxHousing)\n",
    "    .transform(idxHousing)\n",
    "\n",
    "ohHousing.select(\"ocean_proximity\", \"ocean_proximity_ints\", \"ocean_proximity_one_hot\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Pipeline\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. For example, you called the `Imputer`, `VectorAssembler`, and `StandardScaler` from left to right. However, we can use the `Pipeline` class to define a sequence of Transformers/Estimators, and run them in order. A `Pipeline` is an `Estimator`, thus, after a Pipeline's `fit()` method runs, it produces a `PipelineModel`, which is a `Transformer`.\n",
    "\n",
    "Now, let's create a pipeline called `numPipeline` to call the numerical transformers you built above (`imputer`, `va`, and `scaler`) in the right order from left to right, as well as a pipeline called `catPipeline` to call the categorical transformers (`indexer` and `encoder`). Then, put these two pipelines `numPipeline` and `catPipeline` into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|            features|              scaled|ocean_proximity_ints|ocean_proximity_one_hot|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-61.007269596069...|                 3.0|          (4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+--------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numPipeline: org.apache.spark.ml.Pipeline = pipeline_a510eb4e800b\n",
       "catPipeline: org.apache.spark.ml.Pipeline = pipeline_499686bccc5a\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_6a792b1ea8da\n",
       "newHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 15 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numPipeline = new Pipeline()\n",
    "    .setStages(Array(imputer, va, scaler))\n",
    "\n",
    "val catPipeline = new Pipeline()\n",
    "    .setStages(Array(indexer, encoder))\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(numPipeline, catPipeline))\n",
    "\n",
    "val newHousing = pipeline\n",
    "    .fit(renamedHousing)\n",
    "    .transform(renamedHousing)\n",
    "\n",
    "newHousing.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `VectorAssembler` to put all attributes of the final dataset `newHousing` into a big vector, and call the new column `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|features                                                                                                                                                                                                                            |label   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "|[-61.00726959606955,17.734477624640412,3.2577023016083064,0.40337085073160667,0.30758821710917267,0.2843362208866199,0.3295584480852433,4.382095394195227,2.8228125480951665,2.5405867237343416,0.24605655309533123,0.0,0.0,0.0,1.0]|452600.0|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "finalHousing: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 14 more fields]\n",
       "va2: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_7a3a22563892\n",
       "dataset: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val finalHousing = newHousing.drop(\"features\")\n",
    "\n",
    "val va2 = new VectorAssembler()\n",
    "     .setInputCols(Array(\"scaled\", \"ocean_proximity_one_hot\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val dataset = va2\n",
    "    .transform(finalHousing)\n",
    "    .select(\"features\", \"label\")\n",
    "\n",
    "dataset.show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Make a model\n",
    "Here we going to make four different regression models:\n",
    "* Linear regression model\n",
    "* Decission tree regression\n",
    "* Random forest regression\n",
    "* Gradient-booster forest regression\n",
    "\n",
    "But, before giving the data to train a Machine Learning model, let's first split the data into training dataset (`trainSet`) with 80% of the whole data, and test dataset (`testSet`) with 20% of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-62.065401082150...| 94600.0|\n",
      "|[-62.040445150874...| 85800.0|\n",
      "|[-62.020480405854...|111400.0|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-62.040445150874...|103600.0|\n",
      "|[-62.025471592109...| 79000.0|\n",
      "|[-62.005506847089...|106700.0|\n",
      "+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\n",
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainSet.show(3)\n",
    "testSet.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Linear regression model\n",
    "Now, train a Linear Regression model using the `LinearRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-54456.50752335199,-54645.695174988425,14019.701483467456,7785.422929937331,951.870875408345,-45951.7936531568,42995.40213305872,78308.04124420747,6825.410797460786,16778.07279655265,745.5910782686559,-177199.48660577965,-212530.29078463395,-171360.93838384398,-181560.7310198749] Intercept: -2220759.9126649867\n",
      "RMSE: 67703.22767684447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_0e7f5cc9d457\n",
       "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_0e7f5cc9d457\n",
       "trainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@6aa44421\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Train the model\n",
    "val lr = new LinearRegression()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"label\")\n",
    "    .setSolver(\"normal\")\n",
    "    .setMaxIter(10)\n",
    "\n",
    "val lrModel = lr.fit(trainSet)\n",
    "\n",
    "val trainingSummary = lrModel.summary\n",
    "\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `RegressionEvaluator` to measure the root-mean-square-erroe (RMSE) of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|145474.21404122608|103600.0|[-62.040445150874...|\n",
      "|181358.86126578646| 79000.0|[-62.025471592109...|\n",
      "| 211206.8264460913|106700.0|[-62.005506847089...|\n",
      "|194893.46144828526| 90100.0|[-61.985542102068...|\n",
      "| 147764.6627887222| 70000.0|[-61.985542102068...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 68668.7637620722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_f742a8166585\n",
       "rmse: Double = 68668.7637620722\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions on the test data\n",
    "val predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "    .setMetricName(\"rmse\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setLabelCol(\"label\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Decision tree regression\n",
    "Repeat what you have done on Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 172707.9489051095|103600.0|[-62.040445150874...|\n",
      "|143072.61713173264| 79000.0|[-62.025471592109...|\n",
      "| 172707.9489051095|106700.0|[-62.005506847089...|\n",
      "|196663.00598476606| 90100.0|[-61.985542102068...|\n",
      "|153808.83534136545| 70000.0|[-61.985542102068...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 68379.42891496226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_e2fb403b9b88\n",
       "dtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel (uid=dtr_e2fb403b9b88) of depth 5 with 63 nodes\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_80be96b2e4fa\n",
       "rmse: Double = 68379.42891496226\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt = new DecisionTreeRegressor()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "// Train the model\n",
    "val dtModel = dt.fit(trainSet)\n",
    "\n",
    "// Make predictions on the test data\n",
    "val predictions = dtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator()\n",
    "    .setMetricName(\"rmse\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Random forest regression\n",
    "Let's try the test error on a Random Forest Model. Youcan use the `RandomForestRegressor` to make a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|126767.46846147433|103600.0|[-62.040445150874...|\n",
      "| 97010.35444230502| 79000.0|[-62.025471592109...|\n",
      "|132536.23018878163|106700.0|[-62.005506847089...|\n",
      "|185218.23203295693| 90100.0|[-61.985542102068...|\n",
      "|  88185.1643612136| 70000.0|[-61.985542102068...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 54003.24561722442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_143f81a4d873\n",
       "rfModel: org.apache.spark.ml.regression.RandomForestRegressionModel = RandomForestRegressionModel (uid=rfr_143f81a4d873) with 25 trees\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_e0f026d576c9\n",
       "rmse: Double = 54003.24561722442\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestRegressor()\n",
    "    .setMaxDepth(10)\n",
    "    .setNumTrees(25)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "// Train the model\n",
    "val rfModel = rf.fit(trainSet)\n",
    "\n",
    "// Make predictions on the test data\n",
    "val predictions = rfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator()\n",
    "    .setMetricName(\"rmse\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Gradient-boosted tree regression\n",
    "Fianlly, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. Use the `GBTRegressor` to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 128283.8531435173|103600.0|[-62.040445150874...|\n",
      "| 99700.62360043684| 79000.0|[-62.025471592109...|\n",
      "| 128283.8531435173|106700.0|[-62.005506847089...|\n",
      "|121588.88118708531| 90100.0|[-61.985542102068...|\n",
      "| 96931.13282731757| 70000.0|[-61.985542102068...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 60423.81688813691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gb: org.apache.spark.ml.regression.GBTRegressor = gbtr_6031fe610ab4\n",
       "gbModel: org.apache.spark.ml.regression.GBTRegressionModel = GBTRegressionModel (uid=gbtr_6031fe610ab4) with 10 trees\n",
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_2a6e27ab523b\n",
       "rmse: Double = 60423.81688813691\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gb = new GBTRegressor()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setMaxIter(10)\n",
    "\n",
    "// Train the model\n",
    "val gbModel = gb.fit(trainSet)\n",
    "\n",
    "// Make predictions on the test data\n",
    "val predictions = gbModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error\n",
    "val evaluator = new RegressionEvaluator()\n",
    "    .setMetricName(\"rmse\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setLabelCol(\"label\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Hyperparameter tuning\n",
    "An important task in Machine Learning is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression, or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as `CrossValidator`. These tools require the following items:\n",
    "* Estimator: algorithm or Pipeline to tune (`setEstimator`)\n",
    "* Set of ParamMaps: parameters to choose from, sometimes called a \"parameter grid\" to search over (`setEstimatorParamMaps`)\n",
    "* Evaluator: metric to measure how well a fitted Model does on held-out test data (`setEvaluator`)\n",
    "\n",
    "`CrossValidator` begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example with `k=3` folds, `CrossValidator` will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular `ParamMap`, `CrossValidator` computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the 3 different (training, test) dataset pairs. After identifying the best `ParamMap`, `CrossValidator` finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "Below, use the `CrossValidator` to select the best Random Forest model. To do so, you need to define a grid of parameters. Let's say we want to do the search among the different number of trees (1, 5, and 10), and different tree depth (5, 10, and 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 211279.1141024572|103600.0|[-62.040445150874...|\n",
      "|161683.92407050836| 79000.0|[-62.025471592109...|\n",
      "|220893.71698130268|106700.0|[-62.005506847089...|\n",
      "| 308697.0533882615| 90100.0|[-61.985542102068...|\n",
      "|  146975.273935356| 70000.0|[-61.985542102068...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 160076.7025258468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfr_143f81a4d873-maxDepth: 1,\n",
       "\trfr_143f81a4d873-numTrees: 5\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 1,\n",
       "\trfr_143f81a4d873-numTrees: 10\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 1,\n",
       "\trfr_143f81a4d873-numTrees: 15\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 5,\n",
       "\trfr_143f81a4d873-numTrees: 5\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 5,\n",
       "\trfr_143f81a4d873-numTrees: 10\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 5,\n",
       "\trfr_143f81a4d873-numTrees: 15\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 10,\n",
       "\trfr_143f81a4d873-numTrees: 5\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 10,\n",
       "\trfr_143f81a4d873-numTrees: 10\n",
       "}, {\n",
       "\trfr_143f81a4d873-maxDepth: 10,\n",
       "\trfr_143f81a4d873-numTrees: 15\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_8032232b8a6f\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipelin..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(rfModel.maxDepth, Array(1, 5, 10))\n",
    "    .addGrid(rfModel.numTrees, Array(5, 10, 15))\n",
    "    .build()\n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"rmse\")\n",
    "\n",
    "// It seems like we have to create a pipeline for setEstimator to work\n",
    "val pipeline = new Pipeline().setStages(Array(rfModel)) \n",
    "\n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setNumFolds(3)\n",
    "val cvModel = cv.fit(trainSet)\n",
    "\n",
    "val predictions = cvModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Custom transformer\n",
    "At the end of part two, we added extra columns to the `housing` dataset. Here, we are going to implement a Transformer to do the same task. The Transformer should take the name of two input columns `inputCol1` and `inputCol2`, as well as the name of ouput column `outputCol`. It, then, computes `inputCol1` divided by `inputCol2`, and adds its result as a new column to the dataset. The details of implemeting a custom Tranfomer are explained [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types). Please read it before before starting to implement it.\n",
    "\n",
    "First, define the given parameters of the Transformer and implement a method to validate their schemas (`StructType`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------+\n",
      "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+-------------------+-------------------+------------------------+\n",
      "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "housingCol1: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n",
       "housingCol2: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\n",
       "housingExtra: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housingCol1 = housing.withColumn(\"rooms_per_household\", expr(\"total_rooms/households\"))\n",
    "val housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\", expr(\"total_bedrooms/total_rooms\"))\n",
    "val housingExtra = housingCol2.withColumn(\"population_per_household\", expr(\"population/households\"))\n",
    "\n",
    "housingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructField, StructType, DoubleType}\n",
       "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
       "defined trait MyParams\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, DoubleType}\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "\n",
    "trait MyParams extends Params {\n",
    "    final val inputCol1 = new Param[String](this, \"inputCol1\", \"The first input column\")\n",
    "    final val inputCol2 = new Param[String](this, \"inputCol2\", \"The second input column\")\n",
    "    final val outputCol = new Param[String](this, \"outputCol\", \"The output column\")\n",
    "    \n",
    "    protected def validateAndTransformSchema(schema: StructType): StructType = {\n",
    "        // Check that the input type of inputCol1 is a DoubleType\n",
    "        val idx1 = schema.fieldIndex($(inputCol1))\n",
    "        val field1 = schema.fields(idx1)\n",
    "\n",
    "        if (field1.dataType != DoubleType) {\n",
    "          throw new Exception(s\"Input type ${field1.dataType} did not match input type DoubleType\")\n",
    "        }\n",
    "        \n",
    "        // Check that the input type of inputCol2 is a DoubleType\n",
    "        val idx2 = schema.fieldIndex($(inputCol2))\n",
    "        val field2 = schema.fields(idx2)\n",
    "                       \n",
    "        if (field2.dataType != DoubleType) {\n",
    "          throw new Exception(s\"Input type ${field2.dataType} did not match input type DoubleType\")\n",
    "        }\n",
    "        \n",
    "        // Add the return field\n",
    "        schema.add(StructField($(outputCol), DoubleType, false))\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, extend the class `Transformer`, and implement its setter functions for the input and output columns, and call then `setInputCol1`, `setInputCol2`, and `setOutputCol`. Morever, you need to override the methods `copy`, `transformSchema`, and the `transform`. The details of what you need to cover in these methods is given [here](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Transformer\n",
       "import org.apache.spark.ml.util.Identifiable\n",
       "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
       "import org.apache.spark.sql.types.{StructField, StructType, DoubleType}\n",
       "import org.apache.spark.sql.{DataFrame, Dataset}\n",
       "import org.apache.spark.sql.functions.{col, udf}\n",
       "defined class MyTransformer\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Transformer\n",
    "import org.apache.spark.ml.util.Identifiable\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "import org.apache.spark.sql.types.{StructField, StructType, DoubleType}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset}\n",
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "\n",
    "class MyTransformer(override val uid: String) extends Transformer with MyParams {\n",
    "    def this() = this(Identifiable.randomUID(\"columnaddition\"))\n",
    "    \n",
    "    def setInputCol1(value: String): this.type = set(inputCol1, value)\n",
    "    def setInputCol2(value: String): this.type = set(inputCol2, value)\n",
    "    def setOutputCol(value: String): this.type = set(outputCol, value)\n",
    "\n",
    "    override def copy(extra: ParamMap): MyTransformer = {\n",
    "        defaultCopy(extra)\n",
    "    }\n",
    "    \n",
    "    override def transformSchema(schema: StructType): StructType = {\n",
    "        validateAndTransformSchema(schema)\n",
    "    }\n",
    "    \n",
    "    override def transform(dataset: Dataset[_]): DataFrame = {\n",
    "        housing.withColumn(${outputCol}, col(${inputCol1})/ col(${inputCol2}))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, an instance of `MyTransformer`, and set the input columns `total_rooms` and `households`, and the output column `rooms_per_household` and run it over the `housing` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|rooms_per_household|\n",
      "+-------------------+\n",
      "|  6.984126984126984|\n",
      "|  6.238137082601054|\n",
      "|  8.288135593220339|\n",
      "| 5.8173515981735155|\n",
      "|  6.281853281853282|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myTransformer: MyTransformer = columnaddition_d5bb01307905\n",
       "myDataset: Unit = ()\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myTransformer = new MyTransformer()\n",
    "    .setInputCol1(\"total_rooms\")\n",
    "    .setInputCol2(\"households\")\n",
    "    .setOutputCol(\"rooms_per_household\")\n",
    "\n",
    "val myDataset = myTransformer\n",
    "    .transform(housing)\n",
    "    .select(\"rooms_per_household\")\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Custom estimator (predictor)\n",
    "Now, it's time to implement your own linear regression with gradient descent algorithm as a brand new Estimator. The whole code of the Estimator is given to you, and you do not need to implement anything. It is just a sample that shows how to build a custom Estimator.\n",
    "\n",
    "The gradient descent update for linear regression is:\n",
    "$$\n",
    "w_{i+1} = w_{i} - \\alpha_{i} \\sum\\limits_{j=1}^n (w_i^\\top x_j - y_j)x_j\n",
    "$$\n",
    "\n",
    "where $i$ is the iteration number of the gradient descent algorithm, and $j$ identifies the observation. Here, $w$ represents an array of weights that is the same size as the array of features and provides a weight for each of the features when finally computing the label prediction in the form:\n",
    "\n",
    "$$\n",
    "prediction = w^\\top \\cdot\\ x\n",
    "$$\n",
    "\n",
    "where $w$ is the final array of weights computed by the gradient descent, $x$ is the array of features of the observation point and $prediction$ is the label we predict should be associated to this observation.\n",
    "\n",
    "The given `Helper` class implements the helper methods:\n",
    "* `dot`: implements the dot product of two vectors and the dot product of a vector and a scalar\n",
    "* `sum`: implements addition of two vectors\n",
    "* `fill`: creates a vector of predefined size and initialize it with the predefined value\n",
    "\n",
    "What you need to do is to implement the methods of the Linear Regresstion class `LR`, which are\n",
    "* `rmsd`: computes the Root Mean Square Error of a given RDD of tuples of (label, prediction) using the formula:\n",
    "$$\n",
    "rmse = \\sqrt{\\frac{\\sum\\limits_{i=1}^n (label - prediction)^2}{n}}\n",
    "$$\n",
    "* `gradientSummand`: computes the following formula:\n",
    "$$\n",
    "gs_{ij} = (w_i^\\top x_j - y_j)x_j\n",
    "$$\n",
    "* `gradient`: computes the following formula:\n",
    "$$\n",
    "gradient = \\sum\\limits_{j=1}^n gs_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.{Vector, Vectors, Matrices}\n",
       "defined class Instance\n",
       "defined object Helper\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Vector, Vectors, Matrices}\n",
    "\n",
    "case class Instance(label: Double, features: Vector)\n",
    "\n",
    "object Helper extends Serializable {\n",
    "    def dot(v1: Vector, v2: Vector): Double = {\n",
    "        val m = Matrices.dense(1, v1.size, v1.toArray)\n",
    "        m.multiply(v2).values(0)\n",
    "    }\n",
    "\n",
    "    def dot_val(v: Vector, s: Double): Vector = {\n",
    "        val baseArray = v.toArray.map(vi => vi * s)\n",
    "        Vectors.dense(baseArray)\n",
    "    }\n",
    "\n",
    "    def sumVectors(v1: Vector, v2: Vector): Vector = {\n",
    "        val baseArray = ((v1.toArray) zip (v2.toArray)).map { case (val1, val2) => val1 + val2 }\n",
    "        Vectors.dense(baseArray)\n",
    "    }\n",
    "\n",
    "      def fillVector(size: Int, fillVal: Double): Vector = Vectors.dense(Array.fill[Double](size)(fillVal));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "defined class LR\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "\n",
    "class LR() extends Serializable {\n",
    "    def calcRMSE(labelsAndPreds: RDD[(Double, Double)]): Double = {\n",
    "        val regressionMetrics = new RegressionMetrics(labelsAndPreds)\n",
    "        regressionMetrics.rootMeanSquaredError\n",
    "    }\n",
    "  \n",
    "    def gradientSummand(weights: Vector, lp: Instance): Vector = {\n",
    "        val mult = (Helper.dot(weights, lp.features) - lp.label)\n",
    "        val seq = (0 to lp.features.size - 1)\n",
    "            .map(i => lp.features(i) * mult)\n",
    "        \n",
    "        return Vectors.dense(seq.toArray)\n",
    "    }\n",
    "  \n",
    "    def linregGradientDescent(trainData: RDD[Instance], numIters: Int): (Vector, Array[Double]) = {\n",
    "        val n = trainData.count()\n",
    "        val d = trainData.take(1)(0).features.size\n",
    "        var w = Helper.fillVector(d, 0)\n",
    "        val alpha = 1.0\n",
    "        val errorTrain = Array.fill[Double](numIters)(0.0)\n",
    "\n",
    "        for (i <- 0 until numIters) {\n",
    "            val labelsAndPredsTrain = trainData\n",
    "                .map(lp => (lp.label, Helper.dot(w, lp.features)))\n",
    "            errorTrain(i) = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "            val gradient = trainData\n",
    "                .map(lp => gradientSummand(w, lp))\n",
    "                .reduce((v1, v2) => Helper.sumVectors(v1, v2))\n",
    "            val alpha_i = alpha / (n * scala.math.sqrt(i + 1))\n",
    "            val wAux = Helper.dot_val(gradient, (-1) * alpha_i)\n",
    "            w = Helper.sumVectors(w, wAux)\n",
    "        }\n",
    "        (w, errorTrain)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.PredictionModel\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
       "defined class MyLinearModel\n",
       "defined class MyLinearModelImpl\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "\n",
    "abstract class MyLinearModel[FeaturesType, Model <: MyLinearModel[FeaturesType, Model]]\n",
    "    extends PredictionModel[FeaturesType, Model] {\n",
    "}\n",
    "\n",
    "class MyLinearModelImpl(override val uid: String, val weights: Vector, val trainingError: Array[Double])\n",
    "    extends MyLinearModel[Vector, MyLinearModelImpl] {\n",
    "\n",
    "    override def copy(extra: ParamMap): MyLinearModelImpl = defaultCopy(extra)\n",
    "\n",
    "    def predict(features: Vector): Double = {\n",
    "        println(\"Predicting\")\n",
    "        val prediction = Helper.dot(weights, features)\n",
    "        prediction\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Predictor\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.{DataFrame, Dataset}\n",
       "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
       "import org.apache.spark.sql.functions.{col, udf}\n",
       "import org.apache.spark.ml.util.Identifiable\n",
       "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
       "import org.apache.spark.sql.Row\n",
       "defined class MyLinearRegression\n",
       "defined class MyLinearRegressionImpl\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{DataFrame, Dataset}\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "import org.apache.spark.ml.util.Identifiable\n",
    "import org.apache.spark.ml.param.{ParamMap, Param, Params}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "abstract class MyLinearRegression[\n",
    "        FeaturesType,\n",
    "        Learner <: MyLinearRegression[FeaturesType, Learner, Model],\n",
    "        Model <: MyLinearModel[FeaturesType, Model]]\n",
    "    extends Predictor[FeaturesType, Learner, Model] {\n",
    "}\n",
    "\n",
    "class MyLinearRegressionImpl(override val uid: String)\n",
    "        extends MyLinearRegression[Vector, MyLinearRegressionImpl, MyLinearModelImpl] {\n",
    "    def this() = this(Identifiable.randomUID(\"linReg\"))\n",
    "\n",
    "    override def copy(extra: ParamMap): MyLinearRegressionImpl = defaultCopy(extra)\n",
    "  \n",
    "    def train(dataset: Dataset[_]): MyLinearModelImpl = {\n",
    "        println(\"Training\")\n",
    "\n",
    "        val numIters = 10\n",
    "\n",
    "        val instances: RDD[Instance] = dataset.select(\n",
    "            col($(labelCol)), col($(featuresCol))).rdd.map {\n",
    "                case Row(label: Double, features: Vector) =>\n",
    "                    Instance(label, features)\n",
    "        }\n",
    "\n",
    "    val (weights, trainingError) = new LR().linregGradientDescent(instances, numIters)\n",
    "\n",
    "    new MyLinearModelImpl(uid, weights, trainingError)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkDriverExecutionException",
     "evalue": " Execution error",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkDriverExecutionException: Execution error",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1377)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)",
      "  at org.apache.spark.rdd.RDD.take(RDD.scala:1337)",
      "  at LR.linregGradientDescent(<console>:36)",
      "  at MyLinearRegressionImpl.train(<console>:52)",
      "  at MyLinearRegressionImpl.train(<console>:35)",
      "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)",
      "  ... 40 elided",
      "Caused by: java.lang.ArrayStoreException: [LInstance;",
      "  at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:90)",
      "  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:2082)",
      "  at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:59)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1373)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2107)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      ""
     ]
    }
   ],
   "source": [
    "val lr = new MyLinearRegressionImpl()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "\n",
    "val model = lr.fit(trainSet)\n",
    "val predictions = model.transform(trainSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"rmse\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. An End-to-End Classification Test\n",
    "As the last step, you are given a dataset called `data/ccdefault.csv`. The dataset represents default of credit card clients.It has 30,000 cases and 24 different attributes. More details about the dataset is available at `data/ccdefault.txt`. In this task you should make three models, compare their results and conclude the ideal solution. Here are the suggested steps:\n",
    "1. Load the data.\n",
    "2. Carry out some exploratory analyses (e.g., how various features and the target variable are distributed).\n",
    "3. Train a model to predict thetarget variable (risk of `default`).\n",
    "  - Employ three different models (logistic regression, decision tree, and random forest).\n",
    "  - Compare the models' performances (e.g., AUC).\n",
    "  - Defend your choice of best model (e.g., what are the strength and weaknesses of each of these models?).\n",
    "4. What more would you do with this data? Anything to help you devise a better solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data set into a Spark dataframe and print its schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LIMIT_BAL: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- EDUCATION: integer (nullable = true)\n",
      " |-- MARRIAGE: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- PAY_0: integer (nullable = true)\n",
      " |-- PAY_2: integer (nullable = true)\n",
      " |-- PAY_3: integer (nullable = true)\n",
      " |-- PAY_4: integer (nullable = true)\n",
      " |-- PAY_5: integer (nullable = true)\n",
      " |-- PAY_6: integer (nullable = true)\n",
      " |-- BILL_AMT1: integer (nullable = true)\n",
      " |-- BILL_AMT2: integer (nullable = true)\n",
      " |-- BILL_AMT3: integer (nullable = true)\n",
      " |-- BILL_AMT4: integer (nullable = true)\n",
      " |-- BILL_AMT5: integer (nullable = true)\n",
      " |-- BILL_AMT6: integer (nullable = true)\n",
      " |-- PAY_AMT1: integer (nullable = true)\n",
      " |-- PAY_AMT2: integer (nullable = true)\n",
      " |-- PAY_AMT3: integer (nullable = true)\n",
      " |-- PAY_AMT4: integer (nullable = true)\n",
      " |-- PAY_AMT5: integer (nullable = true)\n",
      " |-- PAY_AMT6: integer (nullable = true)\n",
      " |-- DEFAULT: integer (nullable = true)\n",
      "\n",
      "The data set has 30000 data points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [ID: int, LIMIT_BAL: int ... 23 more fields]\r\n",
       "c: Long = 30000\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferschema\", \"true\")\n",
    "    .load(\"data/ccdefault.csv\")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "val c = df.count\n",
    "println(s\"The data set has $c data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the ID variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfDrop: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfDrop = df.drop(\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the following numerical variables:\n",
    "\n",
    "- The amount of given credit and age (LIMIT_BAL)\n",
    "- The age\n",
    "- The bill_amt features\n",
    "- The pay_amt features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+\n",
      "|summary|         LIMIT_BAL|              AGE|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|             30000|            30000|\n",
      "|   mean|167484.32266666667|          35.4855|\n",
      "| stddev|129747.66156720246|9.217904068090155|\n",
      "|    min|             10000|               21|\n",
      "|    max|           1000000|               79|\n",
      "+-------+------------------+-----------------+\n",
      "\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-----------------+----------------+\n",
      "|summary|        BILL_AMT1|        BILL_AMT2|        BILL_AMT3|         BILL_AMT4|        BILL_AMT5|       BILL_AMT6|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-----------------+----------------+\n",
      "|  count|            30000|            30000|            30000|             30000|            30000|           30000|\n",
      "|   mean|       51223.3309|49179.07516666667|       47013.1548| 43262.94896666666|40311.40096666667|      38871.7604|\n",
      "| stddev|73635.86057552966|71173.76878252832|69349.38742703677|64332.856133916444|60797.15577026471|59554.1075367459|\n",
      "|    min|          -165580|           -69777|          -157264|           -170000|           -81334|         -339603|\n",
      "|    max|           964511|           983931|          1664089|            891586|           927171|          961664|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-----------------+----------------+\n",
      "\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|summary|         PAY_AMT1|          PAY_AMT2|         PAY_AMT3|          PAY_AMT4|          PAY_AMT5|         PAY_AMT6|\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "|  count|            30000|             30000|            30000|             30000|             30000|            30000|\n",
      "|   mean|        5663.5805|         5921.1635|        5225.6815| 4826.076866666666| 4799.387633333334|5215.502566666667|\n",
      "| stddev|16563.28035402577|23040.870402057186|17606.96146980311|15666.159744032062|15278.305679144742|17777.46577543531|\n",
      "|    min|                0|                 0|                0|                 0|                 0|                0|\n",
      "|    max|           873552|           1684259|           896040|            621000|            426529|           528666|\n",
      "+-------+-----------------+------------------+-----------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDrop.describe(\"LIMIT_BAL\",\"AGE\").show()\n",
    "\n",
    "df.describe(\"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\").show()\n",
    "\n",
    "df.describe(\"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look into the distributions of the categorical features\n",
    "\n",
    " - Target\n",
    " - Sex\n",
    " - Education\n",
    " - Marriage\n",
    " - Pay_0 - Pay_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|DEFAULT|count|\n",
      "+-------+-----+\n",
      "|      0|23364|\n",
      "|      1| 6636|\n",
      "+-------+-----+\n",
      "\n",
      "+---+-----+\n",
      "|SEX|count|\n",
      "+---+-----+\n",
      "|  2|18112|\n",
      "|  1|11888|\n",
      "+---+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|EDUCATION|count|\n",
      "+---------+-----+\n",
      "|        2|14030|\n",
      "|        1|10585|\n",
      "|        3| 4917|\n",
      "|        5|  280|\n",
      "|        4|  123|\n",
      "|        6|   51|\n",
      "|        0|   14|\n",
      "+---------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|MARRIAGE|count|\n",
      "+--------+-----+\n",
      "|       2|15964|\n",
      "|       1|13659|\n",
      "|       3|  323|\n",
      "|       0|   54|\n",
      "+--------+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|PAY_0|count|\n",
      "+-----+-----+\n",
      "|    0|14737|\n",
      "|   -1| 5686|\n",
      "|    1| 3688|\n",
      "|   -2| 2759|\n",
      "|    2| 2667|\n",
      "|    3|  322|\n",
      "|    4|   76|\n",
      "|    5|   26|\n",
      "|    8|   19|\n",
      "|    6|   11|\n",
      "|    7|    9|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|PAY_2|count|\n",
      "+-----+-----+\n",
      "|    0|15730|\n",
      "|   -1| 6050|\n",
      "|    2| 3927|\n",
      "|   -2| 3782|\n",
      "|    3|  326|\n",
      "|    4|   99|\n",
      "|    1|   28|\n",
      "|    5|   25|\n",
      "|    7|   20|\n",
      "|    6|   12|\n",
      "|    8|    1|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|PAY_3|count|\n",
      "+-----+-----+\n",
      "|    0|15764|\n",
      "|   -1| 5938|\n",
      "|   -2| 4085|\n",
      "|    2| 3819|\n",
      "|    3|  240|\n",
      "|    4|   76|\n",
      "|    7|   27|\n",
      "|    6|   23|\n",
      "|    5|   21|\n",
      "|    1|    4|\n",
      "|    8|    3|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|PAY_4|count|\n",
      "+-----+-----+\n",
      "|    0|16455|\n",
      "|   -1| 5687|\n",
      "|   -2| 4348|\n",
      "|    2| 3159|\n",
      "|    3|  180|\n",
      "|    4|   69|\n",
      "|    7|   58|\n",
      "|    5|   35|\n",
      "|    6|    5|\n",
      "|    1|    2|\n",
      "|    8|    2|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|PAY_5|count|\n",
      "+-----+-----+\n",
      "|    0|16947|\n",
      "|   -1| 5539|\n",
      "|   -2| 4546|\n",
      "|    2| 2626|\n",
      "|    3|  178|\n",
      "|    4|   84|\n",
      "|    7|   58|\n",
      "|    5|   17|\n",
      "|    6|    4|\n",
      "|    8|    1|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|PAY_6|count|\n",
      "+-----+-----+\n",
      "|    0|16286|\n",
      "|   -1| 5740|\n",
      "|   -2| 4895|\n",
      "|    2| 2766|\n",
      "|    3|  184|\n",
      "|    4|   49|\n",
      "|    7|   46|\n",
      "|    6|   19|\n",
      "|    5|   13|\n",
      "|    8|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featuresToDescribe: Array[String] = Array(DEFAULT, SEX, EDUCATION, MARRIAGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featuresToDescribe = Array(\"DEFAULT\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\",\n",
    "                               \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\")\n",
    "\n",
    "for (feat <- featuresToDescribe) {\n",
    "    dfDrop.groupBy(feat)\n",
    "        .count()\n",
    "        .sort(desc(\"count\"))\n",
    "        .show()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "\n",
    "From the tables that are presented above we can derive the following results:\n",
    "\n",
    "1. We have more instances from females (18112) than males (11888)\n",
    "\n",
    "2. The majority of the participants are graduates either from university (14030) or from school (10585), whereas only 4917 participants have graduated from high school. The minority has graduated from other institutes.\n",
    "\n",
    "3. Almost half of the participants are single (15964), whereas 13659 are married and the rest 377 have undefined marital status.\n",
    "\n",
    "4. \"PAY_0\" is the attribute that describes the payment status on September, where as we can see from the measurement scale of the repayment status most of the participants (14737) paid without delay on September (measurement scale = 0), whereas the minority paid after 3 or more months. In the same way we can observe what were the results for the rest months (columns X7 to X11).\n",
    "\n",
    "5. Although in all of those 7 months the majority of the participants paid without delay (measurement scale = 0), there is no other difference in the rest of the scales. This means that the participants have a different behavior between two or more different months, which is reasonable regarding other parameters that can affect their lives. The only conclusion that we can derive, as it has been mentioned above, is that the majority was paying without delay in all these 7 months.\n",
    "\n",
    "6. As we can see from the results above, we have a class imbalance, where the total number of instances of one of our classes (the one with positive labels = 1.0) is far less than the total number of instances of the other class (the one with negative labels = 0.0). More specifically, we have around 22% positive instances (label = 1.0) and 78% negative instances (label = 0.0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the average limit balance for the different sexes, maritial statuses and education levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|SEX|    avg(LIMIT_BAL)|\n",
      "+---+------------------+\n",
      "|  1| 163519.8250336474|\n",
      "|  2|170086.46201413427|\n",
      "+---+------------------+\n",
      "\n",
      "+---------+------------------+\n",
      "|EDUCATION|    avg(LIMIT_BAL)|\n",
      "+---------+------------------+\n",
      "|        1|212956.06991025034|\n",
      "|        6|148235.29411764705|\n",
      "|        3|126550.27049013626|\n",
      "|        5| 168164.2857142857|\n",
      "|        4|220894.30894308942|\n",
      "|        2| 147062.4376336422|\n",
      "|        0|217142.85714285713|\n",
      "+---------+------------------+\n",
      "\n",
      "+--------+------------------+\n",
      "|MARRIAGE|    avg(LIMIT_BAL)|\n",
      "+--------+------------------+\n",
      "|       1|182200.89318398127|\n",
      "|       3| 98080.49535603715|\n",
      "|       2|156413.66073665748|\n",
      "|       0|132962.96296296295|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featuresToDescribe: Array[String] = Array(SEX, EDUCATION, MARRIAGE)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featuresToDescribe = Array(\"SEX\", \"EDUCATION\", \"MARRIAGE\")\n",
    "\n",
    "for (feat <- featuresToDescribe) {\n",
    "    df.groupBy(feat)\n",
    "        .agg(avg(\"LIMIT_BAL\"))\n",
    "        .show()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for correlations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_cors: (features: Array[String], df: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_cors(features: Array[String], df: DataFrame) = {\n",
    "    val va = new VectorAssembler()\n",
    "        .setInputCols(features)\n",
    "        .setOutputCol(\"COMBINED_FEATURES\")\n",
    "\n",
    "    val attributes = va.transform(df)\n",
    "\n",
    "    val Row(coeff: Matrix) = Correlation.corr(attributes, \"COMBINED_FEATURES\").head\n",
    "    val matrixRows = coeff.rowIter.toSeq.map(_.toArray)\n",
    "    val tempDf = spark.sparkContext.parallelize(matrixRows).toDF(\"Row\")\n",
    "\n",
    "    val numOfCols = matrixRows.head.length\n",
    "    val dfCorrelation = (0 until numOfCols).foldLeft(tempDf)((tempDf, num) => \n",
    "        tempDf.withColumn(\"Col\" + num, $\"Row\".getItem(num)))\n",
    "      .drop(\"Row\")\n",
    "\n",
    "    println(s\"The standard correlation coefficients:\\n\")\n",
    "    dfCorrelation.show(false)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard correlation coefficients:\n",
      "\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|Col0              |Col1              |Col2              |Col3              |Col4              |Col5              |\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|1.0               |0.9514836727518164|0.8922785291271811|0.8602721890293089|0.8297786058329933|0.8026501885528523|\n",
      "|0.9514836727518164|1.0               |0.9283262592714868|0.8924822912577247|0.8597783072714432|0.8315935591018226|\n",
      "|0.8922785291271811|0.9283262592714868|1.0               |0.9239694565909823|0.8839096973620095|0.8533200905940505|\n",
      "|0.8602721890293089|0.8924822912577247|0.9239694565909823|1.0               |0.9401344040880051|0.9009409547978421|\n",
      "|0.8297786058329933|0.8597783072714432|0.8839096973620095|0.9401344040880051|1.0               |0.9461968070521906|\n",
      "|0.8026501885528523|0.8315935591018226|0.8533200905940505|0.9009409547978421|0.9461968070521906|1.0               |\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "The standard correlation coefficients:\n",
      "\n",
      "+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|Col0               |Col1               |Col2               |Col3               |Col4               |Col5               |\n",
      "+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|1.0                |0.2855755286868427 |0.25219113895240175|0.19955793117068046|0.1484592750153428 |0.18573525544572722|\n",
      "|0.2855755286868427 |1.0                |0.24477045029284722|0.1801067436456241 |0.18090775259416947|0.15763391627233472|\n",
      "|0.25219113895240175|0.24477045029284722|1.0                |0.216325091700843  |0.15921372030871106|0.1627400332918339 |\n",
      "|0.19955793117068046|0.1801067436456241 |0.216325091700843  |1.0                |0.15183043582617842|0.1578339156013718 |\n",
      "|0.1484592750153428 |0.18090775259416947|0.15921372030871106|0.15183043582617842|1.0                |0.15489552525961597|\n",
      "|0.18573525544572722|0.15763391627233472|0.1627400332918339 |0.1578339156013718 |0.15489552525961597|1.0                |\n",
      "+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "The standard correlation coefficients:\n",
      "\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|Col0              |Col1              |Col2              |Col3              |Col4              |Col5              |\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|1.0               |0.6721643825483117|0.5742450926204353|0.5388406268712332|0.509426063665447 |0.4745530860641512|\n",
      "|0.6721643825483117|1.0               |0.766551682934095 |0.6620671310239535|0.6227802453768725|0.5755008617793068|\n",
      "|0.5742450926204353|0.766551682934095 |1.0               |0.7773588733012698|0.6867745109947861|0.6326835927184404|\n",
      "|0.5388406268712332|0.6620671310239535|0.7773588733012698|1.0               |0.8198353114868195|0.7164494815807879|\n",
      "|0.509426063665447 |0.6227802453768725|0.6867745109947861|0.8198353114868195|1.0               |0.8169001604176755|\n",
      "|0.4745530860641512|0.5755008617793068|0.6326835927184404|0.7164494815807879|0.8169001604176755|1.0               |\n",
      "+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featuresBillAMT: Array[String] = Array(BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6)\r\n",
       "dfCorBillAMT: Unit = ()\r\n",
       "featuresPayAMT: Array[String] = Array(PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\r\n",
       "dfCorPayAMT: Unit = ()\r\n",
       "featuresPay: Array[String] = Array(PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6)\r\n",
       "dfCorPay: Unit = ()\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Find correlations in the bill_amt* features\n",
    "\n",
    "val featuresBillAMT = Array(\"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\")\n",
    "val dfCorBillAMT = find_cors(featuresBillAMT, dfDrop)\n",
    "\n",
    "// Find correlations in the pay_amt* features\n",
    "val featuresPayAMT = Array(\"PAY_AMT1\", \"PAY_AMT2\", \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\")\n",
    "val dfCorPayAMT = find_cors(featuresPayAMT, dfDrop)\n",
    "\n",
    "// Find correlations in the pay_* features\n",
    "val featuresPay = Array(\"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\")\n",
    "val dfCorPay = find_cors(featuresPay, dfDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the correlations\n",
    "\n",
    "Regarding the tables above, which present the correlations in the attributes, we can derive the following results:\n",
    "\n",
    "1. The attributes {\"BILL_AMT1\", ... \"BILL_AMT6\"} represent the amount of bill statements in these 7 months. The correlation in these attributes is high, but gradually it is being slightly decreased. This means that from one month to another one user has  slightly the same expenses, but this is not a behavior that cannot been changed significantly after 6-7 months.\n",
    "\n",
    "2. The attributes {\"PAY_0\", ...\"PAY_6\"} represent the repayment status for every month, which is the delay of the repayment in every month. The correlation here sometimes is really high (~ 0.82), whereas in other cases can be really low (~0.47). This is reasonable, as the payment delay from one month to another is something that can change significantly or not, because it can be affected by some external factors.\n",
    "\n",
    "3. The attributes {\"PAY_AMT1\", ...\"PAY_AMT6\"} represent the amount that has been paid for every month. The correlation here is not high (~0.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the target variable and drop the old target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfTarget: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "dfDropped: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfTarget = dfDrop.withColumnRenamed(\"DEFAULT\", \"label\")\n",
    "\n",
    "val dfDropped = dfTarget.drop(\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the label, feature, categorical and numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colLabel: String = label\r\n",
       "colFeat: Array[String] = Array(LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\r\n",
       "colCat: Array[String] = Array(SEX, EDUCATION, MARRIAGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6)\r\n",
       "colNum: Array[String] = Array(LIMIT_BAL, AGE, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Label columns\n",
    "val colLabel = \"label\"\n",
    "\n",
    "// Feature columns\n",
    "val colFeat = dfDropped.columns.filter(_ != colLabel)\n",
    "\n",
    "// Categorical columns\n",
    "val colCat = Array(\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\",\n",
    "                   \"PAY_4\", \"PAY_5\", \"PAY_6\")\n",
    "\n",
    "// Numerical columns\n",
    "val colNum = Array(\"LIMIT_BAL\", \"AGE\", \"BILL_AMT1\", \"BILL_AMT2\", \"BILL_AMT3\",\n",
    "                   \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", \"PAY_AMT1\", \"PAY_AMT2\",\n",
    "                   \"PAY_AMT3\", \"PAY_AMT4\", \"PAY_AMT5\", \"PAY_AMT6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make categorical features ready for one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df01: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df02: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df21: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df22: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df31: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df32: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df41: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df42: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df51: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 22 more fields]\r\n",
       "df52: org.apache.spark...."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make sure that the categorical variables don't contain any negative values\n",
    "val df01 = dfDropped.na.replace(\"PAY_0\", Map(-1 -> 10))\n",
    "val df02 = df01.na.replace(\"PAY_0\", Map(-2 -> 11))\n",
    "val df21 = df02.na.replace(\"PAY_2\", Map(-1 -> 10))\n",
    "val df22 = df21.na.replace(\"PAY_2\", Map(-2 -> 11))\n",
    "val df31 = df22.na.replace(\"PAY_3\", Map(-1 -> 10))\n",
    "val df32 = df31.na.replace(\"PAY_3\", Map(-2 -> 11))\n",
    "val df41 = df32.na.replace(\"PAY_4\", Map(-1 -> 10))\n",
    "val df42 = df41.na.replace(\"PAY_4\", Map(-2 -> 11))\n",
    "val df51 = df42.na.replace(\"PAY_5\", Map(-1 -> 10))\n",
    "val df52 = df51.na.replace(\"PAY_5\", Map(-2 -> 11))\n",
    "val df61 = df52.na.replace(\"PAY_6\", Map(-1 -> 10))\n",
    "val dfUpdatedCatCols = df61.na.replace(\"PAY_6\", Map(-2 -> 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for LIMIT_BAL : 0\n",
      "Missing values for SEX : 0\n",
      "Missing values for EDUCATION : 0\n",
      "Missing values for MARRIAGE : 0\n",
      "Missing values for AGE : 0\n",
      "Missing values for PAY_0 : 0\n",
      "Missing values for PAY_2 : 0\n",
      "Missing values for PAY_3 : 0\n",
      "Missing values for PAY_4 : 0\n",
      "Missing values for PAY_5 : 0\n",
      "Missing values for PAY_6 : 0\n",
      "Missing values for BILL_AMT1 : 0\n",
      "Missing values for BILL_AMT2 : 0\n",
      "Missing values for BILL_AMT3 : 0\n",
      "Missing values for BILL_AMT4 : 0\n",
      "Missing values for BILL_AMT5 : 0\n",
      "Missing values for BILL_AMT6 : 0\n",
      "Missing values for PAY_AMT1 : 0\n",
      "Missing values for PAY_AMT2 : 0\n",
      "Missing values for PAY_AMT3 : 0\n",
      "Missing values for PAY_AMT4 : 0\n",
      "Missing values for PAY_AMT5 : 0\n",
      "Missing values for PAY_AMT6 : 0\n"
     ]
    }
   ],
   "source": [
    "for (c <- colFeat) {\n",
    "    val missingCount = dfUpdatedCatCols.filter(dfDrop(c).isNull || \n",
    "            dfUpdatedCatCols(c) === \"\" || dfUpdatedCatCols(c).isNaN).count() \n",
    "    printf(\"Missing values for \" + c + \" : \" + missingCount + \"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|SCALED_FEATURES                                                                                                                             |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(14,[0,1,2,3,4,9],[0.15414535998894324,2.603628744963987,0.053139869207970765,0.0435834725779124,0.009935199510232218,0.029903384202815683])|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_b1ef1d317925\r\n",
       "dfToScale: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 23 more fields]\r\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_cf5929c9c1fa\r\n",
       "dfScaled: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 24 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va = new VectorAssembler()\n",
    "    .setInputCols(colNum)\n",
    "    .setOutputCol(\"FEATURES_TO_SCALE\")\n",
    "\n",
    "val dfToScale = va.transform(dfUpdatedCatCols)\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "    .setInputCol(\"FEATURES_TO_SCALE\")\n",
    "    .setOutputCol(\"SCALED_FEATURES\")\n",
    "\n",
    "val dfScaled = scaler\n",
    "    .fit(dfToScale)\n",
    "    .transform(dfToScale)\n",
    "\n",
    "dfScaled.select(\"SCALED_FEATURES\").show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|  PAY_3_one_hot|\n",
      "+---------------+\n",
      "|(11,[10],[1.0])|\n",
      "| (11,[0],[1.0])|\n",
      "| (11,[0],[1.0])|\n",
      "| (11,[0],[1.0])|\n",
      "|(11,[10],[1.0])|\n",
      "| (11,[0],[1.0])|\n",
      "| (11,[0],[1.0])|\n",
      "|(11,[10],[1.0])|\n",
      "| (11,[2],[1.0])|\n",
      "|     (11,[],[])|\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_7081bee6cbcf\r\n",
       "dfHotEncoded: org.apache.spark.sql.DataFrame = [LIMIT_BAL: int, SEX: int ... 33 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(colCat)\n",
    "  .setOutputCols(colCat map (name => s\"${name}_one_hot\"))\n",
    "\n",
    "val dfHotEncoded = encoder.\n",
    "    fit(dfScaled)\n",
    "    .transform(dfScaled)\n",
    "\n",
    "dfHotEncoded.select(\"PAY_3_one_hot\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                              |label|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(91,[0,1,2,3,4,9,15,19,27,38,57,68],[0.15414535998894324,2.603628744963987,0.053139869207970765,0.0435834725779124,0.009935199510232218,0.029903384202815683,1.0,1.0,1.0,1.0,1.0,1.0])|1    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "va: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_942fd5798645\r\n",
       "dataset: org.apache.spark.sql.DataFrame = [features: vector, label: int]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val va = new VectorAssembler()\n",
    "    .setInputCols(Array(\"SCALED_FEATURES\", \"MARRIAGE_one_hot\", \"EDUCATION_one_hot\", \"SEX_one_hot\",\n",
    "                        \"PAY_0_one_hot\", \"PAY_2_one_hot\", \"PAY_3_one_hot\", \"PAY_4_one_hot\",\n",
    "                        \"PAY_5_one_hot\", \"PAY_6_one_hot\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "val dataset = va\n",
    "    .transform(dfHotEncoded)\n",
    "    .select(\"features\", \"label\")\n",
    "\n",
    "dataset.show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data set in a training set (80%) and test set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(91,[0,1,2,3,4,5,...|    0|\n",
      "|(91,[0,1,2,3,4,5,...|    0|\n",
      "|(91,[0,1,2,3,4,5,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(91,[0,1,2,3,4,5,...|    0|\n",
      "|(91,[0,1,2,3,4,5,...|    0|\n",
      "|(91,[0,1,2,3,4,5,...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: int]\r\n",
       "testSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: int]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainSet, testSet) = dataset.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainSet.show(3)\n",
    "testSet.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to measure other metrics: Accuracy, Recall, Precision, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_metrics: (predictions: org.apache.spark.sql.DataFrame, str: String)Unit\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(predictions: DataFrame, str: String)={\n",
    "    \n",
    "    val lp = predictions.select( \"label\", \"prediction\")\n",
    "    val count_total = predictions.count()\n",
    "    val correct = lp.filter($\"label\" === $\"prediction\").count()\n",
    "    val wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n",
    "    val TP = lp.filter($\"prediction\" === 1.0).filter($\"label\" === $\"prediction\").count()\n",
    "    val TN = lp.filter($\"prediction\" === 0.0).filter($\"label\" === $\"prediction\").count()\n",
    "    val FN = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "    val FP = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" === $\"prediction\")).count()\n",
    "\n",
    "    print(\"\\nFor the \")\n",
    "    print(str)\n",
    "    print(\" classifier the following evaluation metrics were computed: \")\n",
    "    val accuracy = correct.toDouble/count_total.toDouble\n",
    "    print(\"\\nThe accuracy is: \")\n",
    "    print(accuracy)\n",
    "\n",
    "    val precision = TP.toDouble / (TP+FP).toDouble\n",
    "    print(\"\\nThe precision is: \")\n",
    "    print(precision)\n",
    "\n",
    "    val recall = TP.toDouble / (TP+FN).toDouble\n",
    "    print(\"\\nThe recall is: \")\n",
    "    print(recall)\n",
    "\n",
    "    val f1_score = (2*precision*recall).toDouble / (precision+recall).toDouble\n",
    "    print(\"\\nThe F1 score is: \")\n",
    "    print(f1_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train various models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best logistic regression model has the following attributes:\n",
      "{\n",
      "\tlogreg_f7e5a1f3a840-aggregationDepth: 2,\n",
      "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.05,\n",
      "\tlogreg_f7e5a1f3a840-family: auto,\n",
      "\tlogreg_f7e5a1f3a840-featuresCol: features,\n",
      "\tlogreg_f7e5a1f3a840-fitIntercept: true,\n",
      "\tlogreg_f7e5a1f3a840-labelCol: label,\n",
      "\tlogreg_f7e5a1f3a840-maxIter: 50,\n",
      "\tlogreg_f7e5a1f3a840-predictionCol: prediction,\n",
      "\tlogreg_f7e5a1f3a840-probabilityCol: probability,\n",
      "\tlogreg_f7e5a1f3a840-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_f7e5a1f3a840-regParam: 0.01,\n",
      "\tlogreg_f7e5a1f3a840-standardization: true,\n",
      "\tlogreg_f7e5a1f3a840-threshold: 0.5,\n",
      "\tlogreg_f7e5a1f3a840-tol: 1.0E-6\n",
      "}\n",
      "The AUC of the best logistic regression model is: 0.7719880518822813\n",
      "For the Logistic Regression classifier the following evaluation metrics were computed: \n",
      "The accuracy is: 0.8274086378737542\n",
      "The precision is: 0.7177541729893778\n",
      "The recall is: 0.35671191553544496\n",
      "The F1 score is: 0.47657430730478595"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegression = logreg_f7e5a1f3a840\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.1,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.05,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.01,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.0,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.1,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.05\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.05,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.05\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam: 0.01,\r\n",
       "\tlogreg_f7e5a1f3a840-regParam: 0.05\r\n",
       "}, {\r\n",
       "\tlogreg_f7e5a1f3a840-elasticNetParam:..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Instantiate the model\n",
    "val lrModel = new LogisticRegression()\n",
    "    .setMaxIter(50)\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"label\")\n",
    "\n",
    "// Define the hyper-parameter grid\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lrModel.regParam, Array(0.1, 0.05, 0.01, 0))\n",
    "    .addGrid(lrModel.elasticNetParam, Array(0.1, 0.05, 0.01, 0))\n",
    "    .build()\n",
    "\n",
    "// The BinaryClassificationEvaluator evaluates on the Area Under the ROC-curve (AUC) metric\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "    \n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(lrModel)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setNumFolds(5)\n",
    "\n",
    "// Perform cross-validation for model selection\n",
    "val cvModel = cv.fit(trainSet)\n",
    "// Predict the labels of the test set\n",
    "val predictions = cvModel.transform(testSet)\n",
    "\n",
    "print(\"The best logistic regression model has the following attributes:\\n\")\n",
    "print(cvModel.bestModel.extractParamMap())\n",
    "\n",
    "print(\"\\nThe AUC of the best logistic regression model is: \")\n",
    "print(evaluator.evaluate(predictions))\n",
    "\n",
    "// Compute other metrics for the classifier\n",
    "compute_metrics(predictions, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best decision model has the following attributes:\n",
      "{\n",
      "\tdtc_19d82a833a7d-cacheNodeIds: false,\n",
      "\tdtc_19d82a833a7d-checkpointInterval: 10,\n",
      "\tdtc_19d82a833a7d-featuresCol: features,\n",
      "\tdtc_19d82a833a7d-impurity: gini,\n",
      "\tdtc_19d82a833a7d-labelCol: label,\n",
      "\tdtc_19d82a833a7d-maxBins: 32,\n",
      "\tdtc_19d82a833a7d-maxDepth: 6,\n",
      "\tdtc_19d82a833a7d-maxMemoryInMB: 256,\n",
      "\tdtc_19d82a833a7d-minInfoGain: 0.0,\n",
      "\tdtc_19d82a833a7d-minInstancesPerNode: 2,\n",
      "\tdtc_19d82a833a7d-predictionCol: prediction,\n",
      "\tdtc_19d82a833a7d-probabilityCol: probability,\n",
      "\tdtc_19d82a833a7d-rawPredictionCol: rawPrediction,\n",
      "\tdtc_19d82a833a7d-seed: 159147643\n",
      "}\n",
      "The AUC of the best decision tree model is: 0.44689869163226875\n",
      "For the Decision Tree classifier the following evaluation metrics were computed: \n",
      "The accuracy is: 0.8260797342192691\n",
      "The precision is: 0.7110438729198184\n",
      "The recall is: 0.35444947209653094\n",
      "The F1 score is: 0.47307498741821846"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtModel: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_19d82a833a7d\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tdtc_19d82a833a7d-impurity: entropy,\r\n",
       "\tdtc_19d82a833a7d-maxDepth: 4,\r\n",
       "\tdtc_19d82a833a7d-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\tdtc_19d82a833a7d-impurity: entropy,\r\n",
       "\tdtc_19d82a833a7d-maxDepth: 5,\r\n",
       "\tdtc_19d82a833a7d-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\tdtc_19d82a833a7d-impurity: entropy,\r\n",
       "\tdtc_19d82a833a7d-maxDepth: 6,\r\n",
       "\tdtc_19d82a833a7d-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\tdtc_19d82a833a7d-impurity: entropy,\r\n",
       "\tdtc_19d82a833a7d-maxDepth: 7,\r\n",
       "\tdtc_19d82a833a7d-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\tdtc_19d82a833a7d-impurity: entropy,\r\n",
       "\tdtc_19d82a833a7d-maxDepth: 8,\r\n",
       "\tdtc_19d82a833a7d-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\tdtc_19d82a833a7d-impurity: entropy,\r\n",
       "\tdtc_19d82a833a7d..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Instantiate the decision tree classifier\n",
    "val dtModel = new DecisionTreeClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(dtModel.impurity, Array(\"entropy\", \"gini\"))\n",
    "    .addGrid(dtModel.maxDepth, Array(4, 5, 6, 7, 8, 9, 10))\n",
    "    .addGrid(dtModel.minInstancesPerNode, Array(1, 2))//, 3))\n",
    "    .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(dtModel)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setNumFolds(5)\n",
    "\n",
    "// Perform cross-validation for model selection\n",
    "val cvModel = cv.fit(trainSet)\n",
    "\n",
    "// Predict the labels of the test set\n",
    "val predictions = cvModel.transform(testSet)\n",
    "\n",
    "print(\"The best decision model has the following attributes:\\n\")\n",
    "print(cvModel.bestModel.extractParamMap())\n",
    "\n",
    "print(\"\\nThe AUC of the best decision tree model is: \")\n",
    "print(evaluator.evaluate(predictions))\n",
    "\n",
    "// Compute other metrics for the classifier\n",
    "compute_metrics(predictions, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best random forest model has the following attributes:\n",
      "{\n",
      "\trfc_cd628e6da4a1-cacheNodeIds: false,\n",
      "\trfc_cd628e6da4a1-checkpointInterval: 10,\n",
      "\trfc_cd628e6da4a1-featureSubsetStrategy: auto,\n",
      "\trfc_cd628e6da4a1-featuresCol: features,\n",
      "\trfc_cd628e6da4a1-impurity: gini,\n",
      "\trfc_cd628e6da4a1-labelCol: label,\n",
      "\trfc_cd628e6da4a1-maxBins: 32,\n",
      "\trfc_cd628e6da4a1-maxDepth: 5,\n",
      "\trfc_cd628e6da4a1-maxMemoryInMB: 256,\n",
      "\trfc_cd628e6da4a1-minInfoGain: 0.0,\n",
      "\trfc_cd628e6da4a1-minInstancesPerNode: 1,\n",
      "\trfc_cd628e6da4a1-numTrees: 50,\n",
      "\trfc_cd628e6da4a1-predictionCol: prediction,\n",
      "\trfc_cd628e6da4a1-probabilityCol: probability,\n",
      "\trfc_cd628e6da4a1-rawPredictionCol: rawPrediction,\n",
      "\trfc_cd628e6da4a1-seed: 207336481,\n",
      "\trfc_cd628e6da4a1-subsamplingRate: 1.0\n",
      "}\n",
      "The AUC of the best random forest model is: 0.7649028540654924\n",
      "For the Random Forest classifier the following evaluation metrics were computed: \n",
      "The accuracy is: 0.8194352159468439\n",
      "The precision is: 0.7414141414141414\n",
      "The recall is: 0.27677224736048267\n",
      "The F1 score is: 0.40307523338824824"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rfModel: org.apache.spark.ml.classification.RandomForestClassifier = rfc_cd628e6da4a1\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: auto,\r\n",
       "\trfc_cd628e6da4a1-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: all,\r\n",
       "\trfc_cd628e6da4a1-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: sqrt,\r\n",
       "\trfc_cd628e6da4a1-minInstancesPerNode: 1\r\n",
       "}, {\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: auto,\r\n",
       "\trfc_cd628e6da4a1-minInstancesPerNode: 2\r\n",
       "}, {\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: all,\r\n",
       "\trfc_cd628e6da4a1-minInstancesPerNode: 2\r\n",
       "}, {\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: sqrt,\r\n",
       "\trfc_cd628e6da4a1-minInstancesPerNode: 2\r\n",
       "}, {\r\n",
       "\trfc_cd628e6da4a1-featureSubsetStrategy: auto,\r\n",
       "\trfc_cd628e6da4a1-minIn..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModel = new RandomForestClassifier()\n",
    "    .setNumTrees(50)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(rfModel.featureSubsetStrategy, Array(\"auto\", \"all\", \"sqrt\"))\n",
    "//     .addGrid(rfModel.maxDepth(4, 5, 6, 7, 8, 9, 10))\n",
    "    .addGrid(rfModel.minInstancesPerNode, Array(1, 2, 3))\n",
    "    .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "val cv = new CrossValidator()\n",
    "    .setEstimator(rfModel)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setNumFolds(5)\n",
    "\n",
    "// Perform cross-validation for model selection\n",
    "val cvModel = cv.fit(trainSet)\n",
    "\n",
    "// Predict the labels of the test set\n",
    "val predictions = cvModel.transform(testSet)\n",
    "\n",
    "print(\"The best random forest model has the following attributes:\\n\")\n",
    "print(cvModel.bestModel.extractParamMap())\n",
    "\n",
    "print(\"\\nThe AUC of the best random forest model is: \")\n",
    "print(evaluator.evaluate(predictions))\n",
    "\n",
    "// Compute other metrics for the classifier\n",
    "compute_metrics(predictions, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we built three different statistical models, as well as we trained and tested all of them using the \"ccdefault.csv\" dataset. The first statistical model was Logistic Regression and it is the appropriate model to use for binary classification. In our case we can use the Logistic Regression for our regression analysis, since our dependent variable is binary/categorical. Moreover, the logistic function is a sigmoid function, or else an S-curve where the return value monotonously is getting increased from 0 to 1 and from 1 to 1. Turning now on how our classifier worked after fine-tuning our predictive model's parameters, we found that the AUC of the best logistic regression model was around 0.772. Also, we computed other metrics, like accuracy, recall, precision and F1-score: <br>\n",
    "The accuracy is: 0.8274 <br>\n",
    "The precision is: 0.7178 <br>\n",
    "The recall is: 0.3567 <br>\n",
    "The F1 score is: 0.4766 <br>\n",
    "\n",
    "As regards the Decision Tree classifier, it is a kind of representation when we want to perform classification. Our dependent variable is categorical and by using a decision tree we actually perform a binary recursive partitioning. So we split our data and we keep splitting it until the end of the tree for every branch. This approach is called \"divide and conquer\", since we split the dataset and we create some smaller subsets until the iteration stops when the algorithm meets a criterion or when all the data in subsets are homogenous. Our decision tree classifier gave us an AUC value of 0.45 for the best decision model. This cannot be, since the AUC is a value in the 0.5-1.0 range. We thus suspect that there is an error in the Spark implementation. To compare the decision tree to the other models, we turn to the other evaluation metrics instead. The results are the followings:<br>\n",
    "The accuracy is: 0.8261<br>\n",
    "The precision is: 0.711<br>\n",
    "The recall is: 0.35445<br>\n",
    "The F1 score is: 0.47308<br>\n",
    "Regarding to the accuracy, precision, recall and F1 score of the decision tree, we can see that the decision tree is good at distinguishing the different classes. Taking into consideration these metrics, we derive that decision tree classifier doesn't underperform in comparison to the logistic regression and random forest classifiers, which was a conclusion that we derived by considering the AUC values. For some reason, we do not get a good AUC value in Spark. \n",
    "\n",
    "Turning now to the last model, we built a random forest classifier, which consists of many decision trees. We build an uncorrelated forest of trees whose result is way more accurate than that of any individual decision tree. Turning now on how our classifier worked after fine-tuning our predictive model's parameters, we found that the AUC of the best random forest model is: 0.7649. Also, we computed other metrics, like accuracy, recall, precision and F1-score:<br>\n",
    "The accuracy is: 0.819<br>\n",
    "The precision is: 0.7414<br>\n",
    "The recall is: 0.2768<br>\n",
    "The F1 score is: 0.4031<br>\n",
    "\n",
    "Briefly now we will explain what the AUC is and why it is a good metric for our classifiers. AUC-ROC or else Area Under the Receiver Operating Characteristics is a metric to evaluate how a statistical model performs. This metric describes if a model is able to give accurate results and to distinguish different classes. The ROC represents the probability and the AUC describes how good is a model so as to separate the different categories. If AUC is close to 1, it means that our model separates the classes pretty well, whereas if AUC is close to 0 the model cannot separate the classes at all. In case AUC is around 0.55, it means that the classifier does not have category separation capability. Let's compare the different AUC score for our models based on what we explained. The Logistic Regression has an AUC score at ~0.77, which means that the model has a high measure of separability comparing with the Decision Tree classifier which has an AUC score around 0.45 and it falls in the worst case senario. Furthemore, the Random Forest outperforms the Decision Tree with an AUC score at 0.765 which is almost similar with the Logistic Regression's score. \n",
    "\n",
    "In this point, we will compare the other metrics that we calculated. Firstly, we will briefly explain what the accuracy, precision, recall and F1-score show and then we will compare the metrics in all our three classifiers. Precision is about how accurate our model is, which means that it gives us an information of how many of the predicted positives are actual positives. It could be a really good metrics, when the costs of False Positive is pretty high. Regarding the recall, it computes the True Positive over the Actual Positives. We should use this metric, when the costs of False Negative is pretty high. It is really useful to compute recall in fraud detection. Finally, we calculate the F1-score, which is a better measure to use if we want to find a balance between Precision and Recall. On the other hand, the metric of accuracy is the number of correctly classified samples over the whole data. \n",
    "\n",
    "Having explained all our metrics, we should now define which metric is the best for us if we want to evaluate our classifiers. F1 Score is a better measure if we want to seek a balance between precision and recall, as well as if there is a class imbalance. Alongside, for calculating accuracy we consider all the correctly classified data, so this metric is more useful when we have almost the same amount of data points in every class. That is the main reason why we prefer to calculate F1 score instead of accuracy. Moreover, accuracy is used when the True Positives and True Negatives are higher weighted and more important, whereas F1-score is prefered when the False Negatives and False Positives are crucial.\n",
    "\n",
    "To sum up, all three classifiers perform really good, but there is something wrong with the computation of the AUC value for decision tree classifier. Regarding the other metrics that we calculated, they are all giving us a good insight of how well the models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
